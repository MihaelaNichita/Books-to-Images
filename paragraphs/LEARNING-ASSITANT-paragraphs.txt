 For making this happen, information needs to be dual coded on its way to one’s brain. In other words, information retrieved from text inputs would be memorized faster and with a higher degree of retention if also transmitted on a visual way, (just like a highway is more inclined to receive permanent maintenance than a country road).
 This is why the goal of the application is to offer the user the possibility to read any piece of text and, have, at the same time, suggestive and memorable images being generated and integrated into his reading experience. The application delivers the text that the user choose to read divided in paragraphs with less than 50 words each.
 The user can visualize the highlighted words that are considered to provide useful information and he is also encouraged to choose the ones he thinks are not only relevant for the paragraph, but also have the capability of triggering a memorable visual marker, amusing whenever possible.
 This paper is attempting to prove that the power of images in the human learning process can be combined with the successful results that deep neural networks are continuously showing, in order to deliver considerable benefits for both machine and human learning. The machine benefits come from the creative human contributions to the dataset used for training the neural network.
 This functionality is implemented by offering the user the possibility to choose the words that seem relevant to him from a given paragraph of text. The inputs provided are further filtered and added to the big collection of pairs containing short paragraphs and related keywords.
 The human can benefit from this collaboration with a machine by receiving the help it has to offer in the process of memorizing information originally transmitted through text in a highly interactive and pleasant visual manner. The kind of help mentioned above refers to providing a visual encoding for the information one wants to learn.
 The application recalls the well-known adage “A picture is worth a thousand words”, by providing images, which often convey the meaning or essence contained in a piece of text. The quality of the images provided depend, however, entirely on previous human creative work, because plenty of data is needed for being able to implement a machine learning algorithm.
 By deconstructing the process of obtaining images, based on the text served as input, it becomes obvious that the first step is to build a system, which understands text. In other words, for being able to provide images related to the message of the text, the system has to extract first that meaningful message.
 On paper, a visual representation of this challenge would look like a sequence of words mapped to another sequence of words. One sequence stand for the list of words in the original text, and the other one depicts the words describing a visual representation of the idea transmitted, or a so-called visual marker.
 In this paper, I propose two versions of the visual marker, the ideal one, built in accordance with memory techniques, and the one that simply merges a list of keywords. The ideal visual marker exhibits a concatenation of words which, as a whole, can arouse amusement or surprise, because this kind of reactions have the power to make the object that caused them unforgettable.
 This is achievable by building connections between new information and the existing one and straighten their degree of strangeness and humor, in the same manner children usually do. Such markers could be gathered only by creating an application dedicated to this exact purpose, which I did, and called it the “Reading Assistant”.
 This application depicts the early version of the “Learning Assistant”. The main difference between the two is that the early one does not integrate images, while the other does. However, there is a significant drawback regarding this way of collecting data. It needs to have at least more than 10 active users in order to collect enough to feed a machine learning model.
This led me to rely on the other type of visual marker for gathering data. Two to three keywords can describe well enough the main idea of a 30 words paragraph. Keywords extraction is not an abundantly explored terrain, therefore, there is no big dataset available and ready to be fed to a machine so it can learn from it. All the dataset entries that I managed to collect come, therefore, from different sources (Fig. 1). I explored a list of websites, which provided amounts of short texts and related keywords in various ways.
 Lifehacker, Huffpost and Howstaffworks are the domains which deliver a good example of the use cases for both the meta tag under “description” name and, more importantly, the one under “keywords” name. The content of the two are eligible to stand for the short paragraph and, respectively, the related keywords. Instagram platform is another valuable data source. I manually built a list of Instagram users that have at least 1K well drafted posts, containing both description and trailing hashtags. For getting the desired content, I used instagram-scraping command line implemented in Python.
 The last and most consistent data source is Twitter. The raw dataset containing pairs of tweets and hashtags is available in a GitHub repository created by the authors of paper [8]. All in one, the final dataset for my application consisted of roughly 5% of “Reading Assistant” data, 20% of Lifehacker, Huffpost and Howstaffworks data, 25% of Instagram posts and 50% of Tweets. The text input has also been the subject of many preprocessing transformations, before being fully prepared for the learning algorithm.
 While searching for a suitable machine learning model, which would bring benefits to the process of implementing the functionalities desired in my application, I encountered plenty of information about how neural networks have led to remarkable progress in many domains such as speech recognition, computer vision, and language processing. .
Machine learning, as a concept, hosts plenty of different models that can be applied to various challenging tasks in real life.  A topical example of successfully switching from traditional methods to more intelligent ones is Google Translate, as it has recently began using neural machine translation, as opposed to its previous statistical methods [14].
 The success of neural networks, and especially the state-of-the-art results of applying them in translation, have conducted me to search for a neural network model suitable for my application. There is an analogy between translation and keywords extraction, because they both can be handled as a sequence to sequence mapping.  .
Figure 1:  Illustration of the main building blocks that the application implies   Therefore, the model I decided to use represents a RNN Encoder-Decoder [1]. The paper introducing it summarizes a successful usage of the model in the framework of Machine Translation. The qualitative analysis of the trained model shows that it captures the linguistic regularities at the word level as well as phrase level.
 Thus, it is suggested that there may be more natural language related applications that may benefit from the RNN Encoder – Decoder. Eventually, some of the related works including human-machine conversations [4] succeeded in proving it right. This application aims the same, the model representing one of the pylons of it, as seen in Fig. 1. .
The approach of this model consists in the fact that it can do more than just mere classification. It is able to map complicated structures to other complicated structures, which proves to be of great use for natural language challenges, such as translation [1] or question answering [4].
 Paper [3] also recommends implementing a mechanism of attention in the decoder, so the decoder decides parts of the source sentence to pay attention to. The implementation of this application will include, therefore, the updated type of decoder. With a well-prepared dataset and the PyTorch library readily available for implementing a Recurrent Neural Network Encoder-Decoder, there is only one thing missing for initiating the training process and that is computing power.
 Training on GPUs (Graphical Processing Units) is a wise move to consider, due to its increased computing speed. Hence, Google Colab is the environment I chose for training the model. When it comes to evaluating the performance of the model on the keyword extraction task, there is no metric that can be used to measure it properly other than human opinion, which is still biased, because even they can have different perspectives over the same piece of text.
 One way of handling the evaluation part of the experiment is to consider that the model fails when output words are not contained in the input paragraph. This approach provides a metric, but is deficient  again, because it penalizes the model even when it outputs a set of words that reveal a meaningful connection with the overall message of the text.
 When testing the model, the examples provided exhibit some behaviors that can be categorized as failures, but there is definitely a number of results that would serve as good visual markers. Under these conditions, the model can be used for the purpose of the application.
 The “Learning Assistant” uses the model to extract relevant keywords from the text and then uses the keywords to query Google Images (Fig. 1). .
	BRIEF INTRODUCTION OF USED TECHNOLOGIESThis chapter introduces both theoretical background and practical tools, which are mandatory to be understood and applied in order to achieve great results in the proposed application. Figure 2 depicts them clustered in separate blocks, depending on the main purpose that each of them serve. Recurrent Neural Networks, Gated Recurrent Unit, Attention Mechanism and Natural Language Processing are all notions strongly related to the neural model I use in the application, namely, a Recurrent Neural Network Encoder-Decoder.
 Since the model in trained over the GPU provided by Google Colab, they are represented one next to another. The last group consist of the technologies used for building the dataset, which are the tkinter Python library, used for building the graphical user interface initially named “Reading Assistant”, and the tools used to extract data from external online sources, which are BeautifulSoup Python package and instagram-scraper Python based command line. .
 Figure 2:  Illustration of used technologies3.1. Programming language: PythonPython is a high-level programming language, thus, it contains words and phrases comprehensible to humans. To translate this high-level language into machine code, Python uses an interpreter, which converts source code into code understood by computers. As an interpreted language, Python uses memory efficiently, is easy to debug, and allows developers to perform complex tasks in a couple of steps and edit code quickly.
 Python also presents the advantages of being an object-oriented language. Objects help developers write better structured code, resulting in software that is easier and faster to build, maintain, and debug. Python is a dynamic language. Eliminating the need to declare variable types results in less code, faster development, and greater flexibility and resilience. .
Python is used for the implementation of each of the app’s requirements. This language is known to be largely chosen over other similar languages, especially in the matter of Machine Learning tasks [16]. In what follows, I will be covering the various benefits that make Python a proper fit for developing applications, which involve working with significant amounts of data. 
Python offers concise and readable code. While complex algorithms and versatile workflows stand behind machine learning and AI, Python’s simplicity allows developers to put all their effort into solving an ML problem instead of focusing on the technical nuances of the language. Python is functional enough to interface with code written in other programming languages. A Python project can be embedded into frameworks of different languages, and vice versa. 
Extensive selection of libraries and frameworks also has a voice in Python’s popularity, considering the fact that it significantly reduces development time. As Implementing AI and ML algorithms turns into a very complex task and requires a lot of time, Python presents ready-made solutions and, this way, saves precious time and effort.
 Some of the libraries for artificial intelligence and machine learning are listed below: Keras, TensorFlow, Scikit-learn and PyTorch for machine learning, NumPy for high-performance scientific computing and data analysis, SciPy for advanced computing, Pandas for general-purpose data analysis and Seaborn for data visualization. .
3.2. LibrariesAs I previously mentioned, Python offers a wide set of libraries for artificial intelligence and machine learning. BeautifulSoup, NLTK, tkinter, PyTorch and Autograd are the fundamental packages for building this application. First of all, for a supervised machine learning application, the dataset used for training the neural network is essential. For the purpose of what this paper advocates, which is providing relevant images based on the text given as input, which implies having at our disposal the relevant keywords served to an image search engine, there is no significant dataset available.
 The challenging tasks of searching for relevant data sources, finding ways of gathering that data and building mechanisms which process the data and serve it in an appropriate format for feeding it into the neural network, is where some of the mentioned libraries prove to be useful. .
	BeautifulSoupConsidering the listed responsibilities regarding data, BeautifulSoup takes charge of gathering the data by crawling relevant web pages. The data source in this case would be the websites that provide articles on various topics. There are some requirements for the chosen websites, thus, significantly limiting the palette size. By inspecting the source code of a few websites, it is inevitable for a developer to realize that very few of them provide a dutiful example of the meta tag use cases.
 The “keywords” attribute, which describes the page content is a rare thing to find. On the other hand, the “description” attribute is widely used. Considering the fact that this application would only benefit from receiving them both, the credits for providing a respectable data source go to the following websites: Lifehacker, Huffpost and Howstuffworks.
 The contents of the meta tags named “description” and “keywords” represent the raw data that qualifies as eligible for serving the roles of “short paragraph” and “relevant keywords”. The decision on whether the data matches the desired pattern (for being fed to the neural network) or not is being taken only after passing it through a pipeline, which will be thoroughly described in the natural language processing section.
	Instagram-ScraperAnother worthy data source for the clause of this application is the social-media platform called Instagram. Its users provide plenty of valuable content that can be sorted and aligned in a manner that is suspected to notably contribute to the dataset. Instagram-scraper is a command-line application written in Python that crawls an Instagram user’s profile page and downloads his/her photos and videos.
 From all the content, media metadata is relevant for this application, provides information about posts, such as text captions and tags, which will eventually stand for the “short paragraph” and “related keywords” in my intelligent application. Instagram-scraper can be used in different ways, depending on the arguments it receives when run. .
	TKINTERThe last data source promotes originality and creativity at a higher degree than the others. It represents the user himself, the one that is supposed to enjoy the benefits of the application. The success of this application actually relies on his input beforehand. The desirable dataset typify a collection of short paragraphs paired with the combinations of words contained in the paragraphs, which describe good visual markers for the paragraphs.
 The job of gathering the desirable dataset is a challenging one, mainly because the idea of having an application generating visual markers for you, as you read along, is novel and, therefore, no other work in this direction has been done so far. However, there is room for pointing at some work that can be adapted to this specific clause, which I am doing in this paper, by using a “sequence to sequence” neural network, originally built for translation.
 The challenge of building the mentioned dataset translates into developing an application that can be seen as an initial version of the “Learning Assistant”, the “Reading Assistant”.  It provides a graphical interface to the reader, due to the functionalities of the tkinter Python package. The user can read the desired piece of text by entering the link of the web page, or any existing book, by selecting it from the list.
 After choosing the desired reading, the buttons and messages on the window guide him on his way to pick up the words he thinks constitute a memorable visual marker (which, when present in the mind’s eye, immediately recalls the important information in the text). .
	NLTK (Natural Language ToolKit)One of the lateral functionalities of the “Reading Assistant” is the highlighting of some words and that is due to the list of stop-words included in NLTK corpora [15]. Those words are more likely to be counted as relevant by the user, providing the fact that they don’t belong to the stop-words category.
 The operation of filtering out useless data is part of what is called preprocessing. In natural language processing, useless words (data), are referred to as stop words. The reason they are useless is that they are very common, thus, they fail to provide any relevant information for machine understanding. NLTK (Natural Language Toolkit) has a list of stop-words stored in 16 different languages.
 The English one contains 127 entries. In a previous state of development, the application also used to tag words with one of the 3 classes: nouns, verbs and adjectives. The resulting visual markers would portray some unforgettable features when diverse instances of the three classes would be aligned together. As a technique that the memory champions frequently use, it is a fact that it is a successful one.
 I finally opted for disclaiming it in this implementation because it would slow down the process of picking the right words. The tagging was done by way of a trained model in the NLTK library. The included POS (Parts Of Speech) tagger was not perfect but it yielded accurate results most of the time.
 The POS tagger in the NLTK library outputs specific tags for certain words. The list of POS tags consists of different abbreviations for different parts of speech. It includes 6 types of verbs, 4 possible variations of nouns and 3 forms of adjectives. An example of POS tagging using the nltk.pos_tag function is illustrated in Fig.
 3. As it is noticeable from the example in Fig. 3, the function needs to be passed a tokenized sentence for tagging. The output  contains a list of tuples, with each second element in a tuple representing the tag assigned to its tuple mate. “VBZ” stands for the present tense, third person, singular number of the verb “to be”.
 “DI” denotes a determiner. “NN” expresses a singular noun. “VBG” stands for the gerund/present participle conjugation of the verb “to go”. “TO” simply stands for “to”. The verb “be” is perceived in its base form (“VB”). “Useful” is correctly labeled as an adjective (“JJ”). “IN” stands for prepositions and “NNS” tags the plural form of a noun.
  Figure 3: Illustration that captures the input passed to pos_tag function and the returned output	PyTorchPyTorch is an open source deep learning platform that provides a rich ecosystem of tools and libraries, which extend PyTorch and support development in computer vision, natural language processing and more [13]. Its integration into Python allows popular libraries and packages to be used for easily writing neural network layers.
 A few PyTorch modules and classes used in this application help creating and training the required neural networks in an elegant way. The most important ones are: torch.nn, torch.optim and torch.autograd. From the multitude of classes provided by the torch.nn module, the ones used in the application are listed below: .
	torch.nn.Module is the base class for all neural network modules. EncoderRNN, DecoderRNN and AttnDecoderRNN all subclass the Module class.	torch.nn.Embedding is a simple lookup table that stores embeddings of a fixed dictionary and size	torch.nn.GRU applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence
From torch.optim, the SGD class is used for implementing stochastic gradient descent as optimizer for both encoder and decoder.	AUTOGRADAutograd is a module belonging to PyTorch, which provides classes and functions implementing automatic differentiation of scalar functions. It was initially implemented through PyTorch Variables as a way to manipulate Tensor metadata [13]. However, recently the mechanism has been updated and it only requires Tensors with the requires_grad=True keyword in order to compute automatic differentiation.
3.3. Development tools: Google Colab Notebook, GPU RuntimeA central processing unit (CPU) is essentially the brain of any computing device, carrying out the instructions of a program by performing control, logical, and input/output (I/O) operations. A graphical processing unit (GPU), on the other hand, has smaller-sized but many more logical cores whose basic purpose is to process a set of simpler and more identical computations in parallel. A GPU is a specialized type of microprocessor.
 It is optimized not only to display graphics, but also to accelerate other geometric calculations. Moreover, most of these computations involve matrix and vector operations, the same type of mathematics that is used in data science. Nvidia creating a parallel computing architecture and platform for its GPUs, called CUDA, gave developers the ability to express simple processing operations in parallel through code. .
Google Colab is a free to use Jupyter notebook that allows developers to use free Tesla K80 GPU. It also offers a total of 12 GB of RAM and can be used up to 12 hours in a row.3.4. Data Science. Supervised Machine Learning
This paper presents both the theoretical and practical side of what building an “intelligent application” means. For understanding what this title connotes, it seems natural to start with explaining the term “artificial intelligence”.Artificial intelligence refers to the simulation of a human brain function by machines. This is achieved by creating an artificial neural network that behaves like a human.
 The primary human functions that an AI machine performs include logical reasoning, learning and self-correction. Given the fact that machines inherently are not smart, to make them so, a lot of computing power and data is needed in order to empower them to simulate human thinking.
 Artificial intelligence includes a broad range of techniques that enable computers to mimic human intelligence. It includes logic, if-then rules, decision trees and machine learning. Machine learning is the ability of machines to learn from the environment and improve themselves from experience, by changing algorithms as they learn more.
 It focuses on enabling algorithms to learn from the data provided, gather insights and make predictions on previously unanalyzed data using the trained model. Machine learning can be performed using multiple approaches. Each of the included models belong to one of the 3 classes: supervised, unsupervised and reinforcement learning.
 The essence of what is called supervised machine learning represents the workflow of collecting relevant data and recognizing patterns in order to generate predictions. In the case of supervised learning, labeled data is used to help machines recognize features and use them for analyzing future data.
 Data science is a concept used to tackle big data and includes data cleansing, preparation, and analysis. Data science is an umbrella term that encompasses data analytics, data mining, machine learning, and several other related disciplines. Some of the processes included in what data science means were borrowed for dealing with data in my application.
 Data mining and data preprocessing are the activities that provide the precious material for machine learning. During development, data sets are used as sources for pattern recognition and to evaluate the performance of a machine learning system. In production, the system is shown new data points often similar but rarely identical to the examples that were available in the phase of development.
3.5   Natural Language ProcessingNatural Language Processing has a seat reserved in the immediate proximity of all the terms that share the word “data” simply because a large fraction of the output of our civilization is encoded in text form. It is considered to be one of the most fruitful applications of machine learning in recent times, given the fact that most state-of-the-art results in natural language processing have been achieved through machine learning [1], [14].
 The existence of overwhelmingly large amounts of text data, combined with the ability of machines to learn, has given rise to machines reading. .
There is plenty of work done in the field of natural language processing and much of it implies neural networks applications that provide impressive results, even with disregarding a lot of prior knowledge. The neural network proposed in [17] serves as a good example. It is applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling and proves to deliver fairly accurate results compared to the state-of-the-art systems. 
In order to achieve outstanding results in the most exciting of the NLP tasks, such as machine translation, information retrieval, text summarization, question answering and opinion mining [18], most NLP projects pass the initially available vocabulary through a pipeline: a series of operations in which the output of one step is the input to the next step.
 The particular sequence of operations threaded below simplifies the data and eliminates irrelevant words: normalization, conversion to lowercase and removing non-alphabetic words, stop words and domain-specific words. .
3.6   Neural NetworksA Neural Network is a class of machine learning algorithms modeled after the human brain. The artificial neuron holds the position of the basic computational unit and networks describe the interconnectivity among each other. The artificial neuron is also called node or unit.
 Figure 3 and Figure 4 illustrate the structures of a biological neuron and, respectively, an artificial neuron. The similarities between the two proves that the computational model is inspired by the way biological neural networks in the human brain process information. .
 Figure 3:  Simplified illustration of two neurons in the human brain forming a synapseAs seen in Fig. 4, the “cell body” receives all the incoming outputs from previous neurons, each output x_i being weighted with the value associated to the “dendrite” it is forming a synapse with, namely w_i. The weight is assigned on the basis of its relative importance to other inputs. The cell is computing then the sum between the received values and adds a bias b to the result.
 If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. A form of activation function f is finally applied to the result and the output is sent to the following neurons. In the computation model, the activation function models the firing rate of a neuron. Some examples of functions frequently used for this role are the sigmoid, hyperbolic tangent and the rectified linear unit. .
 Figure 4:  Mathematical model of an artificial neuronAn artificial neural network is composed of neurons, which form connections among each other. It is an adaptive system that can be trained and, during this process, neurons fire whenever they learn specific patterns from data. It subsequently modifies its parameters (e.g. weights, biases) based on information that flows through the network. 
The second element of structure for a neural network is the layer. A layer includes a number of nodes and belongs to one of the three classes that follow: input, hidden or output layer. No computation is done within the input layer. The related nodes are responsible of passing inputs to the next level. A hidden layer is where processing and computation is done.
 Hidden nodes pass the information to the next hidden layer or to the output layer, where nodes finally use an activation function that maps to the desired output format. The activation function, also called transfer function, has to be nonlinear for allowing the network to compute nontrivial problems using a small number of nodes. Some of the most used classes of neural networks are the Feedforward NNs with two subclasses, the Single-layer Perceptron and the Multi-layer Perceptron, Convolutional NNs and Recurrent NNs.
 In all of them, except the recurrent one, the information moves in only one direction, from the input nodes, through the hidden nodes (if any) all the way to the output nodes. The recurrent neural networks, on the other hand, propagate data both forward and backwards, from later processing stages to earlier stages. Unlike classical feedforward neural networks, RNNs ca use their internal memory to process arbitrary sequences of inputs.
 This makes them suitable for tasks like speech recognition, language modelling, weather forecasting and other general sequence processors. .
3.6.1   Recurrent Neural NetworksA conventional NN has the shortcoming of not being able to use its reasoning about previous inputs in the network to inform later ones, therefore, it lacks persistence. Recurrent neural networks address the issue. They are networks with loops in them, allowing information to persist. Figure 5 depicts an illustration of the way information flows through a RNN.
 It receives an input x_t at time step t, processes it and returns an output, also referred to as hidden state  h_t, which follows two paths: one as output and another one as input to the same RNN at the next time step  t+1. .
 Figure 5:  A recurrent neural networkIf unrolling the loop (Fig. 6), the RNN’s chain-like nature reveals that recurrent neural networks are related to sequences and lists. They are the natural architecture of neural network to be chosen for such data. Recurrent neural networks are a rich class of dynamic models that are able to handle variable-length sequence inputs.
 (1). The activation functions vary from very simple ones, like sigmoid function, to complex gating mechanisms, like LSTMs and GRUs. h_((t))= f (h_((t-1) ),x_t)			                   		    (1).
When an RNN is trained to predict the next symbol in a sequence, the output at each time step t  is a conditional distribution over the next element of that sequence, given its current state h_((t)). Each conditional probability distribution is modelled as seen in Eq. (2). This results in a network learning the probability distribution over a sequence. If a special output symbol is inserted at the end of the sequence (e.
	The problem about RNNs is that they fail to capture long-term dependencies, because the gradients tend to either vanish, or explode (rarely) [11]. When working with long sequences, the effect of long-term dependencies is hidden by the effect of short-term dependencies. As solutions to this issue, two approaches have been largely preferred over others, LSTM (Long Short Term Memory) and GRU (Gating Recurrent Unit).
 They have internal mechanisms called gates that can regulate the flow of information. These gates can learn which data in a sequence is important to keep and which to throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions. GRU is the model used in the proposed application. .
3.6.2   Gating Recurrent Unit	The GRU is used as a solution to the deficiency of the gradient-based optimization method. Considering that it has a simpler architecture compared to LSTM, it allows faster optimization on smaller datasets [11]. The approach consists of designing a sophisticated activation function, by using gating units (Fig. 7).
	The listed equations practically demonstrate the following behavior of the GRU. When the reset gate is close to zero (off), it makes the unit act as if it is reading the first symbol in an input sequence, allowing it to forget the previously computed state.
	  RNN Encoder-DecoderIn the proposed intelligent application, I use a neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed length vector representation, and the other decodes the representation into another variable-length sequence. The model is proposed in paper [1] and proves to achieve very good results for the Machine Translation task, by learning a semantically and syntactically meaningful representation of linguistic phrases.
 (1). After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a context fixed-length vector c  that summarizes the whole input sequence, as illustrated in Fig.8. .
The decoder of the model is another RNN which is trained to generate the output sequence by predicting the next symbol y_t, given the hidden state h_((t)). However, unlike the RNN encoder, both y_t and h_((t)) are also conditioned on y_(t-1) and on the summary c of the input sequence.
 Hence, the hidden state of the decoder at time t  is computed as shown in Eq. (9), and similarly, the conditional distribution of the next symbol is given by Eq. (10), where f and g are activation functions. Function g must produce valid probabilities (e.
 Once the RNN Encoder–Decoder is trained, the model can be used to generate a target sequence given an input sequence. h_((t))= f (h_((t-1)),y_(t-1),c), 	 			   (9).
 The approach insists on consuming all hidden states from the input in order to address the weakening context issue. The technique consists of adding to the original RNN decoder a hidden layer that adaptively remembers and forgets. Figure 9 shows the graphical depiction of the hidden layer attached to the decoder in the model. For avoiding confusion, the hidden states of the decoder use the letter s for notating each of the hidden units. .
 Figure 9:  An illustration of the RNN Encoder-Decoder architecture, the Decoder being updated with attention mechanismIn the model introduced in paper [3], each generated output is not a function of just the final hidden state, but rather a function of all hidden states from the encoder.  In other words, probability is conditioned on a distinct context vector   c_i for each target word   y_i.
The hidden layer computes a categorical distribution over elements from the input sequence to reflect their importance weights. It allows RNN to maintain a variable-length memory, so that elements from the input sequence can be selected by their relevance and merged into the output. Computing attention only requires matrix multiplication, which is highly parallelizable.
 3. APPLICATION DESCRIPTION4.1 ArchitectureThis section describes the developed prototype of the application. Text, model predictions, images and other specific services relating the user-machine interaction are all merged into the “Learning Assistant” application depicted in Fig. 10. The flow of interactions between various components of the application is started by the user requesting a book or a certain article on a web page to read.
 This request follows the purple path illustrated in Fig 10. If the user inserts the address of the desired web page, the request is carried on to the component that is specialized in extracting web content, i.e. the crawlers, for subsequently filtering it and dividing the whole text into paragraphs consisting of less than 50 words each. .
 Figure 10 - Illustration of the interaction between the user and the “Learning Assistant” applicationBy choosing a book from the existing list, since online content is not required, the app retrieves the book content from the local library, which already includes the existing books divided into paragraphs. Whenever the application is started, the first thing it does is checking whether there are any new books, not divided into paragraphs yet, and completes the task, if so. The thread illustrated with purple in the diagram finally reaches its goal of delivering the desired content, divided into paragraphs, to the graphical user interface, so the user can visualize it.
 The burgundy thread starts from the user as well and aims to collect two lists of words, convert them to specific format and send them to a local text file, where all the data from the user is gathered. The first sequence from one entry depicts a list of words from the current paragraph, that are not contained in a predefined list of stop-words, and the second sequence represents a memorable combination of words, summarizing the content of the paragraph, or the so called Visual Marker for the paragraph.
 The RNN Encoder-Decoder is previously trained with a set of 90K pairs of sentences and related keywords, so that it can be used for keywords extraction in the application. The way it provides the desired result is emphasized with the yellow arrows and comments in Fig. 10. For the current paragraph, the list of source words, which are contained in the paragraph and not in the list of stop-words, are served as input to the model. It evaluates the input and returns a sequence of keywords which is further used for querying Google Images.
 The images provided by the online image library are then incorporated into the application, so the user can read and visualize relevant images, contributing to the efficiency of the learning process. .
4.2    Stages of DevelopmentFor a better understanding of all the processes that the development of this application went through, I propose to view it as a sequence of steps. These steps represent the answers to the following questions, generated by starting from the desired output of the application. 
 Figure 11 – Illustration of the stages the proposed application goes through, considering the reverse path of question answering (from goal to input)Since the desired output depicts a collage of suggestive images related to the input text, the very first question that naturally uncovers is:
Q1. Where from and how do I obtain the proper images?	A11. Google Images offers a large variety of images and also provides a command line that can be used in automated processes. 	A12. The proper images can be fetched by using one or more representative words as search queries.
Q2. How do I obtain a combination of representative words for a piece of text?	A2. The proposed solution is to use a sequence to sequence network capable of mapping a list of words from a paragraph to a shorter list of words from the original list (or not), that would describe a representative image for that paragraph.
Q3. Why is the proposed model expected to provide a satisfying result?	A3. The idea of using an Encoder-Decoder Recurrent Neural Network is inspired from the success of the model in fields like Machine Translation [1] and Question Answering [3]. The authors of the cited papers show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. The analogy between a translation task and keyword extraction is obvious, since both refer to a “sequence to sequence” mapping problem.
Q4. How should the data fed into the model look?	A4. One entry from the dataset should contain a source sequence of more than 2 and less than 50 words and a shorter target sequence. Words should not contain any stop-word or other symbols than letters and digits. This is why all the data will be preprocessed.
Also, keywords that make sense together are represented with an underscore in between and are considered to be a single key-phrase.Q5. Which resources of text data to use in order to acquire a large collection of pairscomposed of short paragraphs and memorable keywords?
	A51. The inputs of users using a “Reading Assistant”. The app collects paragraphs together with the corresponding keywords, which users think are relevant	A52. A dataset of 112.5K (sentence, hashtags) unfiltered pairs extracted from Twitter [6]	A53. Captions of Instagram users’ posts that also contain hashtags.
	A54.  Content of web pages that make use of the two meta tags with the name attributes equal to “description” and, respectively, “keywords”.Q6. How to obtain data from the mentioned resources?	A61. The user input can only be obtained by building the stated “Reading Assistant” application.
	A62. Web page content can be collected by using BeautifulSoup crawling specialized package available for Python.	A63. Instagram-scraper represents the solution for downloading Instagram content.In the interest of understanding the workflow of building the application, the questions and answers section was presented in reverse order compared to the actual order of the actions. Therefore, congregating the proper dataset is the first action point to be discussed.
4.3 Obtaining the proper Data Set	In this section and the afferent subsections, all the processes that will be described focus on data. They all aim to maximize its quality and quantity, and refer to one on the four stages listed in Fig. 12. Therefore, sections 4.3.1, 4.3.2 mainly refer to the first stage, sections 4.3.3 describes the second and the third stages from Fig. 12, and the last stage reflect section 4.3.4.
 Figure 12:  Pipeline representing the operations performed on data in order to prepare it for the NNThe only available dataset of short paragraphs and related keywords I could find is the one provided by the others of paper [6]. It consists of 100K pairs of unfiltered tweets and hashtags, which will be reduced to almost half the amount, because it proves to be very noisy. Since at least 100K entries are needed for training the model, the issue has to be addressed by building extensions to the dataset.
 In the matter of quantity, the RNN Encoder-Decoder model proved to provide its best results when trained with datasets counting more than 10M pairs with a vocabulary of nearly 300M words [2]. For working well, models coming from research tend to be complex and require very limited amounts of training data [5], compared to language models for real-world applications. However, not having a large open dataset available for the proposed task makes it challenging to acquire even a few dozens of thousands of entries.
 The task has been completed by building and using crawlers in order to obtain more data, which is subsequently added to the existing dataset. The perfect entry in the dataset would consist of a short text paragraph and a few words from within the paragraph, which, put together, shape a memorable visual marker for that piece of text. Being memorable is synonym to displaying a set of features that are unusual, thus humoristic, for the object or the living being they are assigned to.
 This kind of entries for the dataset can only be achieved by exploiting the inexhaustible source of creativity humans share. .
An example of such a visual marker can be seen in Fig. 13, where the words highlighted in pink were chosen as keywords for the given paragraph. Because the humor comes in many ways, this specific marker is not immediately conceived as amusing. It is the mind’s eye job to make it memorable, by building a story based on the highlighted words and, thus, linking the message of the text with it. For example, one could imagine a man seriously preparing himself for a race, when the moderator starts fooling around and keeps repeating “attention”, then “start”, then “stop”, then “start” again, and finally declares himself the one who is “winning” the competition anyway.
 This sort of story would definitely stick to the memory for a while and would recall the information from the text, or at least some points from where to start digging deeper. .
 Figure 13:  Snippet of the “Reading Assistant” application depicting an example of short paragraph, from Snyder’s book “Save the Cat!”, where the words highlighted in pink were chosen for jointly building a representative visual marker4.3.1 User Input. “READING ASSSTANT” application
The first data source is the one that also impelled the process of developing the graphical side of the application with all the functionalities, except the one providing images. The collected data is fully dependent on the user input and that is the reason why it is critical to create a comfortable and suggestive environment for him.
 Therefore, the list of functionalities that are placed within the user’s reach is the “Reading Assistant” app are illustrated in Fig. 14 and described in what follows. .
 Figure 14:  Capture of the “Reading Assistant” application	Choosing the desired reading and importing it to the app. Pressing “Book” button or “Article” button determines one of the two new windows to pop up:	“Books List” window, for choosing a book from an existing list, as shown in Fig. 15
	The “Articles List” window captured in Fig. 16, which provides a text box for pasting the link of the desired web page	Having the text divided into paragraphs and being able to jump through them, by pressing the button showing the paragraph number. It leads the small window represented in Fig. 17 to pop up.
	Visualizing the highlighted words which aspire to be chosen (are not in the stop-words list)	Sending words to a list of chosen words, by left-clicking them, and also removing them from the list, by right-clicking	Marking the choice as complete by pressing a “Send” button
	Reset the choice that has not been completed and sent yet	The chosen words can also be linked, if they make more sense together than separate (e.g. Fig. 18), by pressing the “Link Words” button and following the instructions	“How to” tab in the main menu provides a short user manual
	A right-clicking on a word in the paragraph causes the small window in the right to show its translation to Romanian  Figure 15 – Capture of the “Books List” window within the “Reading Assistant” application Figure 16:  Capture of the “Articles List” window within the “Reading Assistant” application
     Figure 17:  Capture of the “Go To Paragraph” window within the “Reading Assistant” applicationFigure 18:  An example of linking words within the “Reading Assistant” application The listed functionalities make it possible for avid readers to contribute with their creativity to the collection of data points, by choosing the words that trigger memorable images in their mind’s eye.
 The data sent to a text file consists of the highlighted words, as the source sequence, and the chosen words, as the target sequence. The two are separated by a tab and the words within the sequences have white spaces between them.
 The initial goal was to have a set of an adjective, a noun and a verb and maybe another word in between for each paragraph. I thought this set of 3 or 4 words would serve as a solid query for image searching.
  A piece of text representing a random fact serves as a good example: .
“In Switzerland it is illegal to own just one guinea pig. This is because guinea pigs are social animals, and they are considered victims of abuse if they are alone.”Following the (adjective, noun, verb) pattern, an output could be: “abuse social guinea_pigs”. The image that these words trigger is quite dramatic and is also capable of causing certain feelings, thus, it is memorable. On the other hand, this same piece of text provides an insight about how much time one can spend trying to choose the best combination.
 This is one of the reasons I disclaimed this approach. The other one is that image libraries are not very responsive to a query consisting of more than two words. You would rarely find an image, which precisely depicts the object or animal wanted, looking exactly the way the adjective describes it and performing the nominated action. It is an easy task for humans to bring those 3 words together in an image, but the degree of success for a machine is not expected to be high, unless it would aim to draw it from scratch [8].
Nevertheless, an early state of the application still suggested combinations of words following that pattern, when pressing the “hint” button. The parts of speech were identified by calling a POS (Parts Of Speech) tagger function included in the NLTK library Python provides.  Eventually, the functionality proved to be of no use and I decided to eliminate it. Therefore, the app collects paragraphs together with the corresponding keywords, which users think are relevant.
 The only restriction for the user is not to exceed the limit of nine words per paragraph, in which case a window with a warning message immediately pops-up (Fig. 19). .
 Figure 19:  Capture of the warning shown when the user attempts to add the tenth word to the list within the “Reading Assistant” applicationThe application described serves its purpose very well in matter of quality. However, quantitatively, there is no way of gathering the amount of data required for training the model in a short amount of time and with no more than 3 users who don’t read for a whole day and don’t do it daily.
 This is a huge drawback, which yields the demand for alternatives regarding data sources. .
4.3.2 Building CrawlersThis brings me to the process of building crawlers, which allows me to obtain valuable text sequences from certain web pages. Crawling usually refers to dealing with large datasets by developing your own bots, which crawl to the deepest contents of the web pages.
The requisites those websites are expected to hold consist of two meta tags with the name attributes equal to “description” and, respectively, “keywords”. Whereas “keywords” are not plentifully used, there is not as big of a choice I had. The three websites that I managed to crawl and that provide a fair amount of “keywords” per web page on average, are named as follows: Lifehacker, Huffpost and Howstuffworks.
The meta tag under the name “description” is providing, for all the web pages that I have inspected, a very consistent summary of the page content. The words extracted from the “keywords” meta tag refer to the whole page content as well. This means there are words in the set that are not contained in the description. Thus, among other preprocessing work, these words need to be eliminated.
The third spot where I could find an opportunity to extract keywords is Instagram. The users that are active on the platform have the potential to provide plenty of valuable content, which can be sorted and processed in a manner that is suspected to notably contribute to the dataset. Instagram-scraper is a command-line application written in Python that crawls an Instagram user’s profile page or a hashtag and downloads the corresponding photos and videos.
 One can choose whether to crawl the posts of a user or the ones that contain a specific hashtag, by using the right arguments when running the command. The whole command text I used in order to obtain the posts metadata for a list of users is as follows:.
 “instagram-scraper -f users-to-scrape.txt -u mihaela.tgr -p <password> --media_metadata --media-types none --no-check-certificate” 	Along with other content, media metadata provides information about posts, such as text captions and tags, which will eventually stand for the “short paragraph” and “related keywords” in my intelligent application. I used instagram-scraper to crawl all the posts provided by a list of, so called, “Instagram Influencers”. Their collections include an average of 1500 posts. After visiting a few of their posts and gaining the certainty that they tend to use both elaborate sentences and hashtags as captions to their posts (Fig.
 20), I added them to the list. Running the command line mentioned above along with a set of arguments results in a json file for each user, where the desired content is stored. The content is easily restored into python variables using the loads() function available in the json library Python has to offer. .
From all these sources I managed to collect 25.3K of (short paragraph, keywords) pairs which have been partially cleaned on the way.4.3.3 Preparing the dataDepending on the source the data has been extracted from, there is a series of operations through which the initial vocabulary is passed. However, the particular sequence of operations threaded below have been applied on all of them before being fed to the model: conversion to lowercase, removing non-alphabetic words, stop words and punctuation, and also filtering out entries that contain more words in the target sequence than in the source sequence.
 For the data obtained from websites that have a blog kind of architecture, only the content of two meta tags is of interest. These are the meta tags under the names “description” and “keywords”. The text sequence extracted from “keywords” might contain words that are not in the “description”. They need to be eliminated and only after that, the pair is good to be appended to the text file containing the dataset.
 For the data obtained from Instagram, there is a different approach on ensuring that the keywords match existing words in the description, because of the multiple words that can be all bounded in one hashtag. The technique consists of checking for each word in the description to be found in the string of hashtags and appending it to the keywords list, only if it is found.
 Another thing to note is that in case hashtags are placed in the middle of the description, they are considered as both part of the description and keywords. The dataset containing 100K pairs of unfiltered tweets and hashtags. Even though the hashtags in the target sequence of each pair already satisfy the condition of being contained in the text, I still needs to filter out plenty of elements producing noise in the dataset.
 The non-Latin characters were filtered using regular expressions Python module. Then, all the URL links were removed, since they give no textual content. Tweets that start with the “@username” are generally considered replies and have a conversional nature more than topical nature. Therefore, they were also removed. I could detect other noisy entries, like repeating sequences of characters, which I removed, by building a script which detect certain features.
 However, Spanish words and other unidentified sources of noise are still in the dataset. An important observation delivered in paper [1] is that the RNN Encoder–Decoder prefers shorter phrases in general. This determined me to establish a limit for each entry in the dataset, which is currently set to 50 words per sequence. After preprocessing the data, it is all written to the text file, from where it will be retrieved and prepared for the neural network.
 Table 1 summarizes the results of all the operations performed over the initial data regarding quantity. Table 1: The counts for data entries before and after preprocessing.
	Before Preprocessing (Pairs)	After Preprocessing (Pairs)Twitter	112.5K 	70.2K The rest	25.3K 	22.7K TOTAL	137.8K	92.9KBefore delivering the data to the words embedding function, there is a final checking point to pass through, in order to make sure that no wrong data is delivered to the words embedding function. The process for preparing the data at this point consists of first reading the text file and splitting it into lines, then splitting lines into pairs, normalizing text and finally making word lists.
 Normalizing refers to turning Unicode characters to ASCII. After fully preprocessing the whole set of data, the counts for each class of entities is as follows. .
Sentence Pairs: 92928Source Words: 53125Target Words 21009	Words Embedding		In an RNN model, word embeddings are introduced to represent the semantics of words. Since plain text cannot be used in a neural network, words need to be encoded into vectors. I will be representing each word as a one-hot encoded vector, or a giant vector of zeros except for a single one at the index of the word, as shown in Fig.
20. The size of the vector corresponds to the number of unique words existing in the dataset.  .
Figure 20 – Graphical representation of the one-hot vector embedding 4.4   Applying the RNN Encoder-Decoder4.4.1 Training the ModelWith a dataset of 92.9K of entries being available and prepared for feeding the described RNN Encoder-Decoder, the training process can start. I trained the captured data based on the recurrent neural networks with the stochastic gradient descent algorithm as optimizer [12], using Pytorch. The original code that implements the model and the train function comes from the Practical PyTorch tutorial on translation with a sequence to sequence network and attention.
 Attention requires a maximum input sentence length for the calculation of attention weights on the hidden intermediate layer. I set it to 50 because the data already fits in it, hence there will be no sentence lost. For a relatively small set of data like the one I managed to build, 256 hidden units and a one layer GRU for each neural network was enough. .
Given the details on the model architecture, after 600K iterations, which is the equivalent of 6.5 epochs, the average loss decreased to the value of 0.95. After that, by analyzing the loss function every 5K iterations, no significant improvement could be observed. The training process on Tesla K80 GPU lasted for 200 minutes.
4.4.1 Experimental Results	Assuming the small dataset that could be used for training the networks, the expectations for the model accuracy are not very high. However, the fact that it outputs words, that are, most of the time contained in the input paragraph, or at least have a semantic connection, proves that, even in these harsh conditions, it behaves well.
A measurable evaluation was note performed, because, first of all, there is no way of measuring the performance other than human opinion, and human opinions are always biased. On the other hand, the system is built for helping them, so their opinion is crucial. A simplistic way of measuring it’s accuracy would be to count the model’s output as good if it contains words from the input sequence, and bad otherwise.
 However, the second reason why, in this case, a measurable evaluation is not recommended is because of the small size of the dataset. Splitting it into a train set and a test set would diminish its performance. Therefore, I decided not to sacrifice valuable training data for measuring the performance that would be definitely smaller otherwise. Some of the examples below can describe the general behavior of the model in the matter of keywords or visual markers extraction.
 Most of the outputs prove to be much shorter (Fig. 21) or too generic (Fig. 22) compared to what a human would extract as keywords. However, for the purpose of this application, short is not a problem, because the images introduced in the next chapter might compensate the missing details. A phenomenon that can be observed frequently is that the generated words are not contained in the original sequence, but it proves to carry a semantical connection with the words in the input.
 Figure 23 serves as a good example, where  ‘focus’, ‘effective’, ‘energy’ makes the network think of yoga, which human might do as well. .
 Figure 21: Example of a short output Figure 22: Example of a summarizing generic output Figure 23: Example of an output, which consists of keywords that are not existent in the input4.4.1 Integrating the model with the application	With the model tested, it can be integrated with the application, so its behavior can be observed in the environment it was meant for.  Merging the “Reading Assistant” application with the trained model is an easy task to do, since both have been implemented in Python.
 The saved model is loaded from a function in the application source code. The evaluation function of the model is called whenever the user opens a book or changes the paragraph. The result, with an example of the model’s output can be seen in Fig. 24, where pink highlights stand for the user choice in matter of keywords, the model’s choice being placed on the bottom-right side of the window. .
One thing to note is that, since the model performs better on smaller sentences, before calling the model’s evaluation function for the current paragraph, I sliced the words extracted from the paragraph into smaller sequences of 15 words or less. 
 Figure 24:  Capture of the “Learning Assistant” with the keywords generated by the model on the bottom-right side4.5   Creating the Image Slideshow4.5.1 Image Search. Google Images	Initially I explored various online image libraries, which are rich in beautiful high-quality artistic pictures, but they proved to be not exactly appealing when trying to automate the process of downloading them, or at least not such as Google is. It offers a command line, which can be called from Python and downloads the images resulted from the search operated on the keywords given as input.
 This facilitates the process of passing the model outputs to the search engine. Because queries bigger than 2 words hardly return the desired output, and the model frequently returns 1 to 3 words per paragraph, I considered that, for now, a one word query is the most convenient to use. 4.5.2 Integrating Visual Markers into “Learning Assistant”.
	Bringing the images to the user implies working with an image specialized module from Python, called “Image”. Each time the users changes the paragraph, the keywords generated by the model are passed to google_images_download command, together with the value representing the number of pictures to download, which is three, and the jpg format. It downloads the images in a folder named after the keywords, so the second time the keyword is encountered, it skips the downloading.
 When it comes to creating the slideshow, one of the three random pictures is shown in the frame reserved for images. An example of the results the application provides is shown in Figure 25.  	.
Figure 24:  Capture of the “Learning Assistant” with the image slideshow  4. FUTURE IMPROVEMENTSFirst of all, the application is built upon the idea of continuous improvement, by encouraging the user to create visual markers every time he uses the map. The input from the user aims to bring the true value to the model, only after collecting more evidence of his creativity and, therefore, updating the data set. The aspects of scalability, connection to a server and a database, and many other mandatory requirements for an application in order to be widely used, were not approached in the current version of the application.
 This is a matter of future as well. Text summarizing would bring an additional improvement to the application, especially in this case, with the method used to extract keywords working better for short sequences of text. .
Developing a game in which players are encouraged to associate an amusing odd image to a given piece of text, and describe the details of it in a few words. Players could be asked to provide three words indicating an object or a living being, an action, and, respectively, a feature. This also could be represented as a combination of a noun, an adjective, and a verb, or a person, an action and an object. On the other hand, the player could be given the freedom to describe the image he’s thinking of in as much detail as he wants and even be rewarded for additional details.
 This way, another module of the learning assistant application will be in charge of extracting the words of interest and let them populate the improved dataset. The game itself would also fall in creativity games category, the user being motivated to play it for either improving his creativity or for testing it. .
This game would induce a huge step forward in making the application actually simulate the human imagination, because, with a dataset providing labels (features) instead of words found in the source text, the results will actually appear to be original and random, just like human creativity.
Another point to be considered in the follow up improvement of the application would be to take the learning and memorizing performances to the next level and make the application integrate all the other fast and efficient memorization techniques and provide a full Learning Assistant service, with personalized features. Some of them would be: Memory Palaces, Chunking, Mind Mapping, Number Memorization Systems, PAO (Person Action Object) System and others. They all rely on the power of visual memory. 
Another change to consider would be to provide an even more interactive way to dual-code the information in a text by means of generating animations. The cited paper [8] introduces a promising perspective over how the future of this project looks like. By applying the Deep Recurrent Attentive Writer (DRAW) neural network architecture, the application could generate realistic images without being limited to what it finds in a library.
6. CONCLUSIONS	To conclude over the work that I have done in order to obtain a modest version of what this optimistic application aims to be, I must mention that I dedicated most of my time to continuously search for valuable data, update the dataset, train the model, analyze the results and engage in the same process again. Continuous improvement has been at the core of the process all the time.
 The research done along the way and the successful applications of neural networks on natural language understanding serve as a proof that great results can be achieved with a solid amount of data, and this is the motivation for the application I propose. As I previously mentioned, this application started as an optimistic idea. The process of developing it conducted me through exploring and sorting the data sources available, filtering data and understanding the forms it can take, whether it is hidden in an Instagram post or under a meta tag not a lot of people use.
 By studying various applications of recurrent neural networks and applying the RNN Encoder-Decoder on a novel task, the idea of machines helping people in the learning process became more tangible than it was at the beginning. In other words, this application started as an optimistic idea and managed to reach the point where it is an optimistic prototype.
I believe that one of the most important ability a human has is to learn and that, as opposed to the machines that do the work for us, the “Learning Assistant” is the kind of machine that is capable of making us better learners, by using visualization techniques. 
In this work, I showed that the RNN Encoder-Attention-Decoder model used in translation, can be adapted to other tasks that imply language understanding. Given the fact that automatic keyword assignment is a research topic that has received less attention than it deserves, the small dataset is the main cause of the outputs that can be labeled as wrong.
 However, I believe it showed good results, given the poor conditions provided. Even if the model does not output words from the input sequence, for the purpose of the application, they are useful as well, because they still have a meaningful connection with the content from the input text. .
To add the last note, I can say this paper shows the struggles and the results of exploring a novel idea, working with data and preparing it for a recurrent neural network that is proven to be a succesful one in matter of natural language processing tasks and integrating them all for the benefit of the human learning.
